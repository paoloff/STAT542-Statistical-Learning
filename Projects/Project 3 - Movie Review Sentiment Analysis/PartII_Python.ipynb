{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Modelling in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.a Processing the reviews and obtaining final vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import string\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, import and load the vocabulary with 1005 words to a list (the number of terms will be trimmed down to 980 later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.read_csv(\"myvocab2.csv\")\n",
    "word_list = list(words['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, pre-process the text: removing all punctuations and replacing underscores with blank space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabR = []\n",
    "for word in word_list:\n",
    "    vocabR.append(word.translate(str.maketrans('_',' ','')).lower())\n",
    "vocab2R = []\n",
    "for word in vocabR:\n",
    "    vocab2R.append(word.translate(str.maketrans('','',string.punctuation)).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we read all review texts from \"alldata.tsv\" and remove punctuation. We also had to remove words ending with \"br\" since it is a html code and not part of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"alldata.tsv\")\n",
    "sentiment_list = list(df['sentiment'])\n",
    "corpus = []\n",
    "\n",
    "for document in df['review']:\n",
    "    clean = document.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "    words = clean.split()\n",
    "    filtered = []\n",
    "    for word in words:\n",
    "        if word[len(word) - 2] == 'b' and word[len(word) - 1] == 'r':\n",
    "                word = word[0:(len(word) - 2)]\n",
    "        filtered.append(word)\n",
    "    new_string = ' '.join(filtered)\n",
    "    corpus.append(new_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will vectorize all reviews. Each review is mapped to a vector with 18613240 features. The value of each feature is how many time the N-gram from N = 1 to 4 occurs in that review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range = (1,4))\n",
    "Xc = vectorizer.fit_transform(corpus)\n",
    "names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, from these 18613240 features, we will only keep the ones that are inside the vocabulary found in R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = []\n",
    "\n",
    "for i in range(18613240):\n",
    "    if names[i] in vocab2R:\n",
    "        inds.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of terms of the vocabulary in Python is 980:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Python dictionary has 980 terms\n"
     ]
    }
   ],
   "source": [
    "print('The Python dictionary has',len(names[inds]),'terms')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the final vocabulary to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vocab = pd.DataFrame(names[inds])\n",
    "final_vocab.to_csv('final_vocab.cvs', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.b Neural Network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python model is a neural network. To run the final model, we need the inputs X and outputs y.\n",
    "\n",
    "The inputs are the columns in the vectorized review matrix which correspond to the terms in the vocabulary. The output is the sentiment of each review. Later we split X and Y into train and test data.\n",
    "\n",
    "Both X and y are transformed to numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(Xc[:,inds].toarray())\n",
    "y = np.array(sentiment_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's separate X and y intro train and test data. Let's take the first split, which is found by taking the review indices in the first column of 'project3_splits.csv':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df = pd.read_csv('project3_splits.csv')\n",
    "test_ind = np.array(list(train_test_df['split_1']))-1\n",
    "train_ind = np.array(list(set(list(range(0,50000)))-set(test_ind)))\n",
    "traindata = X[train_ind]\n",
    "trainlabels = y[train_ind]\n",
    "testdata = X[test_ind]\n",
    "testlabels = y[test_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's run the neural network (NN) model. It has one hidden layer with 20 inputs, with ReLU activation. The output is a single number between 0 and 1, obtained by the sigmoid function. We train all NN for 20 epochs. Each split takes only a few seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "782/782 - 1s - loss: 0.3181 - auc: 0.9429 - 1s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "782/782 - 1s - loss: 0.2420 - auc: 0.9648 - 641ms/epoch - 820us/step\n",
      "Epoch 3/20\n",
      "782/782 - 1s - loss: 0.2337 - auc: 0.9673 - 642ms/epoch - 822us/step\n",
      "Epoch 4/20\n",
      "782/782 - 1s - loss: 0.2293 - auc: 0.9685 - 632ms/epoch - 808us/step\n",
      "Epoch 5/20\n",
      "782/782 - 1s - loss: 0.2263 - auc: 0.9691 - 648ms/epoch - 829us/step\n",
      "Epoch 6/20\n",
      "782/782 - 1s - loss: 0.2230 - auc: 0.9698 - 643ms/epoch - 822us/step\n",
      "Epoch 7/20\n",
      "782/782 - 1s - loss: 0.2211 - auc: 0.9704 - 634ms/epoch - 811us/step\n",
      "Epoch 8/20\n",
      "782/782 - 1s - loss: 0.2191 - auc: 0.9709 - 638ms/epoch - 816us/step\n",
      "Epoch 9/20\n",
      "782/782 - 1s - loss: 0.2179 - auc: 0.9712 - 648ms/epoch - 828us/step\n",
      "Epoch 10/20\n",
      "782/782 - 1s - loss: 0.2169 - auc: 0.9714 - 651ms/epoch - 833us/step\n",
      "Epoch 11/20\n",
      "782/782 - 1s - loss: 0.2156 - auc: 0.9717 - 650ms/epoch - 831us/step\n",
      "Epoch 12/20\n",
      "782/782 - 1s - loss: 0.2144 - auc: 0.9720 - 657ms/epoch - 840us/step\n",
      "Epoch 13/20\n",
      "782/782 - 1s - loss: 0.2136 - auc: 0.9723 - 642ms/epoch - 821us/step\n",
      "Epoch 14/20\n",
      "782/782 - 1s - loss: 0.2126 - auc: 0.9726 - 638ms/epoch - 816us/step\n",
      "Epoch 15/20\n",
      "782/782 - 1s - loss: 0.2119 - auc: 0.9726 - 690ms/epoch - 883us/step\n",
      "Epoch 16/20\n",
      "782/782 - 1s - loss: 0.2108 - auc: 0.9730 - 647ms/epoch - 827us/step\n",
      "Epoch 17/20\n",
      "782/782 - 1s - loss: 0.2102 - auc: 0.9732 - 645ms/epoch - 825us/step\n",
      "Epoch 18/20\n",
      "782/782 - 1s - loss: 0.2099 - auc: 0.9733 - 644ms/epoch - 824us/step\n",
      "Epoch 19/20\n",
      "782/782 - 1s - loss: 0.2093 - auc: 0.9734 - 644ms/epoch - 824us/step\n",
      "Epoch 20/20\n",
      "782/782 - 1s - loss: 0.2084 - auc: 0.9736 - 647ms/epoch - 828us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dc71698a30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(980,), activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', metrics=[metrics.AUC()])\n",
    "model.fit(traindata, trainlabels, epochs=20, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate the AUC of the trained NN on the test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 651us/step\n",
      "0.9658117636913647\n"
     ]
    }
   ],
   "source": [
    "Ypred = model.predict(testdata)\n",
    "print(roc_auc_score(testlabels, Ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all 5 splits, the AUC is bigger than 0.96.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
